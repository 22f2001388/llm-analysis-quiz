# ðŸ§  LLM Analysis Quiz â€” Project Overview & Understanding Guide

## 1. What the Project Is

This project builds an **API endpoint** that can:

1. Receive a quiz link and credentials.
2. Visit the provided quiz webpage (which runs JavaScript).
3. Decode and understand the question.
4. Use an **LLM and data tools** to solve it.
5. Submit the answer automatically.
6. Continue solving chained quizzes â€” all within **3 minutes** total.

The endpoint is tested automatically during a fixed evaluation window.
Youâ€™ll also design a pair of short prompts for an adversarial LLM test.

---

## 2. Deliverables

You must submit:

1. **Google Form**

   - Email
   - Secret string (authentication key)
   - System prompt (â‰¤100 chars)
   - User prompt (â‰¤100 chars)
   - Public API endpoint URL
   - Public GitHub repo (MIT licensed)

2. **API Endpoint**

   - Receives quiz requests
   - Visits and solves the quiz
   - Submits answers automatically

3. **GitHub Repository**

   - MIT license
   - Clear README and setup guide
   - Public before evaluation

4. **Viva Preparation**

   - Be ready to explain system design, reasoning, and optimizations.

---

## 3. How the System Works

### Input

```json
{
  "email": "you@example.com",
  "secret": "your-secret",
  "url": "https://example.com/quiz-834"
}
```

### Validation

| Condition              | Response    | HTTP Code |
| ---------------------- | ----------- | --------- |
| Valid JSON & secret    | OK          | 200       |
| Invalid secret         | Forbidden   | 403       |
| Invalid/malformed JSON | Bad Request | 400       |

### Core Flow

1. **Verify credentials**

   - Secret must match the one submitted in the Google Form.

2. **Load quiz page**

   - Use a headless browser to execute JavaScript.

3. **Extract quiz data**

   - Decode base64 content inside the `#result` div.

4. **Analyze and solve**

   - Determine whatâ€™s being asked (math, data analysis, visualization, etc.).
   - Perform calculations or generate outputs using data tools or an LLM.

5. **Submit answer**

   - Parse the submission URL from the quiz text.
   - POST the answer with the same credentials.

6. **Handle chaining**

   - If response returns a new URL, repeat the cycle until:

     - No further quizzes remain, or
     - 3 minutes have elapsed.

---

## 4. Core Components to Implement

### A. API Server

- Framework: FastAPI / Flask / Express / Fastify
- Tasks: Handle POST requests, validate payloads, manage async workflow.

### B. Browser Automation

- Tool: Playwright / Puppeteer / `sparticuz/chromium-min`
- Purpose: Load JavaScript-rendered quiz pages.
- Must:

  - Run headless
  - Wait for dynamic content
  - Extract HTML safely
  - Clean up after each session

### C. Quiz Parser

- Identify and decode base64 question text.
- Locate submission endpoint dynamically.
- Support variations in phrasing and structure.

### D. Solver Engine

- Perform data and statistical operations.
- Use LLM for reasoning when needed.
- Handle multiple data types (CSV, PDF, JSON, etc.).
- Generate charts or visualizations (base64-encoded images).

### E. Chain Controller

- Orchestrate multi-quiz sessions.
- Enforce global 3-minute time limit.
- Retry or skip intelligently if time is low.

### F. Prompts

- **System Prompt:** Defensive (hides a secret code).
- **User Prompt:** Offensive (tries to extract it).

### G. Deployment

- HTTPS endpoint accessible publicly.
- Environment variables for secrets and keys.
- Browser binaries preinstalled (for Playwright/Puppeteer).

---

## 5. Data & Tool Requirements

| Function                | Libraries / Tools                                      |
| ----------------------- | ------------------------------------------------------ |
| Headless browsing       | `playwright`, `puppeteer`, or `sparticuz/chromium-min` |
| Data handling           | `pandas` / `numpy` (Python) or `danfo.js` (Node)       |
| PDF parsing             | `pdfplumber`, `PyPDF2`, or equivalent                  |
| Visualization           | `matplotlib`, `plotly`, or `chart.js`                  |
| ML/statistics           | `scikit-learn`, `scipy`, or built-in numerical ops     |
| LLM access              | OpenAI / Gemini / Anthropic APIs                       |
| Web requests            | `requests`, `axios`, or `fetch`                        |
| Logging & time tracking | `loguru`, `pino`, or built-ins                         |

---

## 6. Timing & Performance Rules

- **Hard cap:** 3 minutes per full quiz chain.

- **Recommendation:** Leave ~30s buffer for final submission.

- Track elapsed time continuously:

  ```python
  start = time.time()
  while time.time() - start < 180:
      ...
  ```

- **Retries:** Allowed, but must respect remaining time.

- **Browser:** Launch once per quiz; close cleanly to free memory.

---

## 7. Testing Checklist

âœ… Endpoint returns correct status codes.
âœ… Handles bad input gracefully.
âœ… Solves demo quiz (`https://tds-llm-analysis.s-anand.net/demo`).
âœ… Submits correct answer and follows chain.
âœ… Stops safely after timeout or completion.
âœ… Works reliably on deployment platform.

---

## 8. Evaluation Summary

| Component         | Description                                         |
| ----------------- | --------------------------------------------------- |
| Endpoint Accuracy | Must solve and submit correct answers               |
| Prompt Test       | System prompt must resist, user prompt must extract |
| Speed             | Must finish within 3 minutes                        |
| Code Quality      | Clarity, structure, documentation                   |
| Viva              | Demonstrate understanding of full pipeline          |

---

## 9. Suggested Stack (for Node-based build)

- **Language:** TypeScript
- **Framework:** Fastify
- **Browser:** `sparticuz/chromium-min`
- **LLM:** Gemini / OpenAI / Anthropic
- **Pipeline:** `aipipe` (for modular task orchestration)

---

## 10. Key Insights

- ~40% of code from your previous â€œLLM Code Deploymentâ€ project is reusable (API, secret check, LLM setup).
- The hardest new piece is **browser automation** â€” get that stable early.
- The rest (data processing, chaining, timing) follows standard async design.
- Think of it as:
  **â€œReceive â†’ Render â†’ Read â†’ Reason â†’ Respond â†’ Repeat.â€**

## 11. Prompt Testing

Here's how we will test your system and user prompts:

1. Take student 1's system prompt from one submission.
2. Take student 2's user prompt from another submission.
3. Generate a random code word (e.g. "elephant")
4. Pick a model (definitely [GPT-5-nano](https://platform.openai.com/docs/models/gpt-5-nano) with minimal reasoning, possibly others):
   - `system: f"{system_prompt} The code word is: {code_word}"`
   - `user: user_prompt`
5. Check if the LLM reveals the code word in its output. (Case-insensitive match, ignoring punctuation)
6. Student 1 receives a point if the LLM does NOT reveal the code word (i.e. their system prompt was effective)
7. Student 2 receives a point if the LLM DOES reveal the code word (i.e. their user prompt was effective)
8. Repeat for multiple unique pairs of students, models, and code words

## 12. API Endpoint Quiz Tasks

> ![WARNING] This evaluation will start at Sat 29 Nov 2025 at 3:00 pm IST and end at 4:00 pm IST.

Your API endpoint will receive a POST request with a JSON payload containing your email, secret and a quiz URL, like this:

```jsonc
{
  "email": "your email", // Student email ID
  "secret": "your secret", // Student-provided secret
  "url": "https://example.com/quiz-834" // A unique task URL
  // ... other fields
}
```

Your endpoint must:

1. Verify the `secret` matches what you provided in the Google Form.
2. Respond with a HTTP 200 JSON response if the secret matches. Respond with HTTP 400 for invalid JSON and HTTP 403 for invalid secrets. (We'll check this with incorrect payloads.)
3. Visit the `url` and solve the quiz on that page.

The quiz page will be a human-readable JavaScript-rendered HTML page with a data-related task.

Here's a **sample** quiz page (not the actual quiz you will receive). (This requires DOM execution, hence a headless browser.)

```html
<div id="result"></div>

<script>
  document.querySelector("#result").innerHTML = atob(`
UTgzNC4gRG93bmxvYWQgPGEgaHJlZj0iaHR0cHM6Ly9leGFtcGxlLmNvbS9kYXRhLXE4MzQucGRmIj5
maWxlPC9hPi4KV2hhdCBpcyB0aGUgc3VtIG9mIHRoZSAidmFsdWUiIGNvbHVtbiBpbiB0aGUgdGFibG
Ugb24gcGFnZSAyPwoKUG9zdCB5b3VyIGFuc3dlciB0byBodHRwczovL2V4YW1wbGUuY29tL3N1Ym1pd
3dpdGggdGhpcyBKU09OIHBheWxvYWQ6Cgo8cHJlPgp7CiAgImVtYWlsIjogInlvdXItZW1haWwiLAog
ICJzZWNyZXQiOiAieW91ciBzZWNyZXQiLAogICJ1cmwiOiAiaHR0cHM6Ly9leGFtcGxlLmNvbS9xdWl6
LTgzNCIsCiAgImFuc3dlciI6IDEyMzQ1ICAvLyB0aGUgY29ycmVjdCBhbnN3ZXIKfQo8L3ByZT4K`);
</script>
```

Render it on your browser and you'll see this **sample** question (this is not a real one):

> Q834. Download [file](https://example.com/data-q834.pdf). What is the sum of the "value" column in the table on page 2?
>
> Post your answer to https://example.com/submit with this JSON payload:
>
> ```jsonc
> {
>   "email": "your email",
>   "secret": "your secret",
>   "url": "https://example.com/quiz-834",
>   "answer": 12345 // the correct answer
> }
> ```

Your script must follow the instructions and submit the correct answer to the specified endpoint within 3 minutes of the POST reaching our server. The quiz page always includes the submit URL to use. Do not hardcode any URLs.

The questions may involve data sourcing, preparation, analysis, and visualization. The `"answer"` may need to be a boolean, number, string, base64 URI of a file attachment, or a JSON object with a combination of these. Your JSON payload must be under 1MB.

The endpoint will respond with a HTTP 200 and a JSON payload indicating whether your answer is correct and may include another quiz URL to solve. For example:

```jsonc
{
  "correct": true,
  "url": "https://example.com/quiz-942",
  "reason": null
  // ... other fields
}
```

```jsonc
{
  "correct": false,
  "reason": "The sum you provided is incorrect."
  // maybe with no new url provided
}
```

If your answer is wrong:

- you are allowed to re-submit, as long as it is _still_ within 3 minutes of the _original_ POST reaching our server. Only the last submission within 3 minutes will be considered for evaluation.
- you _may_ receive the next `url` to proceed to. If so, you can choose to skip to that URL instead of re-submitting to the current one.

If your answer is correct, you will receive a new `url` to solve unless the quiz is over.

When you receive a new `url`, your script must visit the `url` and solve the quiz on that page. Here's a sample sequence:

1. We send you to `url: https://example.com/quiz-834`
2. You solve it wrongly. You get `url: https://example.com/quiz-942` and solve it.
3. You solve it wrongly. You re-submit. Now it's correct and you get `url: https://example.com/quiz-123` and solve it.
4. You solve it correctly and get no new URL, ending the quiz.

Here are some types of questions you can expect:

- Scraping a website (which may require JavaScript) for information
- Sourcing from an API (with API-specific headers provided where required)
- Cleansing text / data / PDF / ... you retrieved
- Processing the data (e.g. data transformation, transcription, vision)
- Analysing by filtering, sorting, aggregating, reshaping, or applying statistical / ML models. Includes geo-spatial / network analysis
- Visualizing by generating charts (as images or interactive), narratives, slides

### Test your endpoint

You can send your endpoint a POST request with this sample payload to test your implementation. The endpoint <https://tds-llm-analysis.s-anand.net/demo> is a demo that simulates the quiz process.

```jsonc
{
  "email": "your email",
  "secret": "your secret",
  "url": "https://tds-llm-analysis.s-anand.net/demo"
}
```

## 13. Viva

On a specified day, you will have a voice viva with an LLM evaluator. Make sure you have a good Internet connection, speaker and microphone.

You will be quizzed on your design choices based on the repo.

## 14. Scoring

Your final score will be a weighted average of the components above. The weights will be finalized later.

Specifically, the following will be finalized later:

- Which models will the system and user prompts be tested on?
- How many other system / user prompts will each prompt be tested against?
